{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征提取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 从类别变量中提取特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用类DicVectorizere 进行one -hot 编码转换\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "onehot_encoder = DictVectorizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "X = [\n",
    "    {'city':'New York'},\n",
    "    {'city':'San Franciso'},\n",
    "    {'city':'Chapel Hill'}\n",
    "]\n",
    "print(onehot_encoder.fit_transform(X).toarray())  # toarray()方法将fit_transform后的结果转换为array数组\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征标准化 \n",
    "———— tandardScaler类、RobustScalar类、preprocessing类的scale方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.         -0.70710678 -1.38873015  0.52489066  0.59299945 -1.35873244]\n",
      " [ 0.         -0.70710678  0.46291005  0.87481777  0.81537425  1.01904933]\n",
      " [ 0.          1.41421356  0.9258201  -1.39970842 -1.4083737   0.33968311]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "X = np.array([\n",
    "    [0., 0., 5., 13., 9., 1.],\n",
    "    [0., 0., 13., 15., 10., 15.],\n",
    "    [0., 3., 15., 2., 0., 11.]\n",
    "])\n",
    "print(preprocessing.scale(X))  # preprocessing类的scale（）函数可单独对任何轴进行标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RobustScalar类是StandardScaler类的另一个选择。\n",
    "# StandardScaler类会在每个实例值上减去特征均值，然后除以特征值标准差。\n",
    "# obustScalar类会减去中位数，然后除以四分位差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 从文本中提取特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词袋模型\n",
    "1.最常用的文本表示法，可看作是one-hot编码的一种扩展，对文本中关注的**每一个词创建一个特征**     \n",
    "\n",
    "2.使用一个**特征向量**表示每个文档，其中的每个元素和词表的一个单词相对应"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用一个包含两个文档的语料库来检验词包模型（语料库是一个文档的集合）\n",
    "corpus = [\n",
    "    'UNC played Duke in basketball',\n",
    "    'Duke lost the basketball game'\n",
    "]  # 10个单词，包含8个独特单词,即由包含八个元素的特征向量进行表示，元素数量为向量维度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 1 0 1 0 1]\n",
      " [1 1 1 0 1 0 1 0]]\n",
      "{'unc': 7, 'played': 5, 'duke': 1, 'in': 3, 'basketball': 0, 'lost': 4, 'the': 6, 'game': 2}\n"
     ]
    }
   ],
   "source": [
    "# CountVectorizer类\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer() # 创建实例,实例将用于将文本转换为数值向量。\n",
    "print(vectorizer.fit_transform(corpus).todense()) # 如果某个词汇在文档中出现，对应的列值为1，否则为0。\n",
    "# todense()方法将转换后的稀疏矩阵转换为密集矩阵（如果需要），这样更容易阅读\n",
    "# 在scikit-learn中，许多文本处理和特征提取方法（如CountVectorizer）默认返回稀疏矩阵。\n",
    "print(vectorizer.vocabulary_) #打印CountVectorizer的词汇表，这是一个包含所有在语料库中出现过的词汇的列表。词汇表的顺序与fit_transform方法返回的向量中的列顺序相对应。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 0 1 0 1 0 0 1]\n",
      " [0 1 1 1 0 1 0 0 1 0]\n",
      " [1 0 0 0 0 0 0 1 0 0]]\n",
      "{'unc': 9, 'played': 6, 'duke': 2, 'in': 4, 'basketball': 1, 'lost': 5, 'the': 8, 'game': 3, 'ate': 0, 'sandwich': 7}\n"
     ]
    }
   ],
   "source": [
    "# 再增加一个文档\n",
    "corpus.append('I ate a sandwich')\n",
    "print(vectorizer.fit_transform(corpus).todense())\n",
    "print(vectorizer.vocabulary_) # 发现共有十个独特的单词。\n",
    "# 'I' 和 'a' 没有匹配正则表达式，因此没有被提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between 1st and 2nd documents: [[2.44948974]]\n",
      "Distance between 1st and 3rd documents: [[2.64575131]]\n",
      "Distance between 2nd and 3rd documents: [[2.64575131]]\n"
     ]
    }
   ],
   "source": [
    "# 通过L^2范数，计算文档之间的特征向量的相似度\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "# 计算第一个文档和第二个文档的L^2范数\n",
    "print('Distance between 1st and 2nd documents:',euclidean_distances(X[0], X[1]))\n",
    "# 计算第一个文档和第三个文档的L^2范数\n",
    "print('Distance between 1st and 3rd documents:',euclidean_distances(X[0], X[2]))\n",
    "# 计算第二个文档和第三个文档的L^2范数\n",
    "print('Distance between 2nd and 3rd documents:',euclidean_distances(X[1], X[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between 1st and 2nd documents: 2.449489742783178\n",
      "Distance between 1st and 3rd documents: 2.6457513110645907\n",
      "Distance between 2nd and 3rd documents: 2.6457513110645907\n"
     ]
    }
   ],
   "source": [
    "# 或者\n",
    "\n",
    "# 计算文档之间的L^2范数距离\n",
    "distances = euclidean_distances(X)\n",
    "\n",
    "# 打印文档之间的距离\n",
    "print('Distance between 1st and 2nd documents:', distances[0, 1])\n",
    "print('Distance between 1st and 3rd documents:', distances[0, 2])\n",
    "print('Distance between 2nd and 3rd documents:', distances[1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征提取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 从类别变量中提取特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用类DicVectorizere 进行one -hot 编码转换\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "onehot_encoder = DictVectorizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "X = [\n",
    "    {'city':'New York'},\n",
    "    {'city':'San Franciso'},\n",
    "    {'city':'Chapel Hill'}\n",
    "]\n",
    "print(onehot_encoder.fit_transform(X).toarray())  # toarray()方法将fit_transform后的结果转换为array数组\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征标准化 \n",
    "———— tandardScaler类、RobustScalar类、preprocessing类的scale方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.         -0.70710678 -1.38873015  0.52489066  0.59299945 -1.35873244]\n",
      " [ 0.         -0.70710678  0.46291005  0.87481777  0.81537425  1.01904933]\n",
      " [ 0.          1.41421356  0.9258201  -1.39970842 -1.4083737   0.33968311]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "X = np.array([\n",
    "    [0., 0., 5., 13., 9., 1.],\n",
    "    [0., 0., 13., 15., 10., 15.],\n",
    "    [0., 3., 15., 2., 0., 11.]\n",
    "])\n",
    "print(preprocessing.scale(X))  # preprocessing类的scale（）函数可单独对任何轴进行标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RobustScalar类是StandardScaler类的另一个选择。\n",
    "# StandardScaler类会在每个实例值上减去特征均值，然后除以特征值标准差。\n",
    "# obustScalar类会减去中位数，然后除以四分位差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 从文本中提取特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词袋模型\n",
    "1.最常用的文本表示法，可看作是one-hot编码的一种扩展，对文本中关注的**每一个词创建一个特征**     \n",
    "\n",
    "2.使用一个**特征向量**表示每个文档，其中的每个元素和词表的一个单词相对应，使用一个二元值表示特征向量的每个元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用一个包含两个文档的语料库来检验词包模型（语料库是一个文档的集合）\n",
    "corpus = [\n",
    "    'UNC played Duke in basketball',\n",
    "    'Duke lost the basketball game'\n",
    "]  # 10个单词，包含8个独特单词,即由包含八个元素的特征向量进行表示，元素数量为向量维度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 1 0 1 0 1]\n",
      " [1 1 1 0 1 0 1 0]]\n",
      "{'unc': 7, 'played': 5, 'duke': 1, 'in': 3, 'basketball': 0, 'lost': 4, 'the': 6, 'game': 2}\n"
     ]
    }
   ],
   "source": [
    "# CountVectorizer类\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer() # 创建实例,实例将用于将文本转换为数值向量。\n",
    "print(vectorizer.fit_transform(corpus).todense()) # 如果某个词汇在文档中出现，对应的列值为1，否则为0。\n",
    "# todense()方法将转换后的稀疏矩阵转换为密集矩阵（如果需要），这样更容易阅读\n",
    "# 在scikit-learn中，许多文本处理和特征提取方法（如CountVectorizer）默认返回稀疏矩阵。\n",
    "print(vectorizer.vocabulary_) #打印CountVectorizer的词汇表，这是一个包含所有在语料库中出现过的词汇的列表。词汇表的顺序与fit_transform方法返回的向量中的列顺序相对应。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 0 1 0 1 0 0 1]\n",
      " [0 1 1 1 0 1 0 0 1 0]\n",
      " [1 0 0 0 0 0 0 1 0 0]]\n",
      "{'unc': 9, 'played': 6, 'duke': 2, 'in': 4, 'basketball': 1, 'lost': 5, 'the': 8, 'game': 3, 'ate': 0, 'sandwich': 7}\n"
     ]
    }
   ],
   "source": [
    "# 再增加一个文档\n",
    "corpus.append('I ate a sandwich')\n",
    "print(vectorizer.fit_transform(corpus).todense())\n",
    "print(vectorizer.vocabulary_) # 发现共有十个独特的单词。\n",
    "# 'I' 和 'a' 没有匹配正则表达式，因此没有被提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between 1st and 2nd documents: [[2.44948974]]\n",
      "Distance between 1st and 3rd documents: [[2.64575131]]\n",
      "Distance between 2nd and 3rd documents: [[2.64575131]]\n"
     ]
    }
   ],
   "source": [
    "# 通过L^2范数，计算文档之间的特征向量的相似度\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "# 计算第一个文档和第二个文档的L^2范数\n",
    "print('Distance between 1st and 2nd documents:',euclidean_distances(X[0], X[1]))\n",
    "# 计算第一个文档和第三个文档的L^2范数\n",
    "print('Distance between 1st and 3rd documents:',euclidean_distances(X[0], X[2]))\n",
    "# 计算第二个文档和第三个文档的L^2范数\n",
    "print('Distance between 2nd and 3rd documents:',euclidean_distances(X[1], X[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between 1st and 2nd documents: 2.449489742783178\n",
      "Distance between 1st and 3rd documents: 2.6457513110645907\n",
      "Distance between 2nd and 3rd documents: 2.6457513110645907\n"
     ]
    }
   ],
   "source": [
    "# 或者\n",
    "\n",
    "# 计算文档之间的L^2范数距离\n",
    "distances = euclidean_distances(X)\n",
    "\n",
    "# 打印文档之间的距离\n",
    "print('Distance between 1st and 2nd documents:', distances[0, 1])\n",
    "print('Distance between 1st and 3rd documents:', distances[0, 2])\n",
    "print('Distance between 2nd and 3rd documents:', distances[1, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 停用词过滤\n",
    "去除大部分文档中常见的单词，如：限定词'the''a''an',助动词'do''be''will',介词'on''around''beneath'等等\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 0 0 1 0 1]\n",
      " [0 1 1 1 1 0 0 0]\n",
      " [1 0 0 0 0 0 1 0]]\n",
      "{'unc': 7, 'played': 5, 'duke': 2, 'basketball': 1, 'lost': 4, 'game': 3, 'ate': 0, 'sandwich': 6}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "    'UNC played Duke in basketball',\n",
    "    'Duke lost the basketball game',\n",
    "    'I ate a sandwich'\n",
    "] \n",
    "# stop_words 参数接受的值应该是 ‘english’、一个包含停用词的列表，或者 None\n",
    "vectorizer = CountVectorizer(stop_words= 'english')  # 用英语的默认停用词列表———停用限定词'the''a''an',助动词'do''be''will',介词'on''around''beneath'等等\n",
    "print(vectorizer.fit_transform(corpus).todense())  \n",
    "print(vectorizer.vocabulary_)  # 打印vocabulary_属性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词干提取和词形还原"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 0]\n",
      " [1 0 0 1]]\n",
      "{'gathering': 0, 'ingredients': 1, 'sandwich': 2, 'wizards': 3}\n"
     ]
    }
   ],
   "source": [
    "# 创建一个由两个文档组成的语料库\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "    'I am gathering ingredients for the sandwich.',\n",
    "    'There were many wizards at the gathering.'\n",
    "]\n",
    "vectorizer = CountVectorizer(binary=True, stop_words='english') # 停用词过滤\n",
    "print(vectorizer.fit_transform(corpus).todense())\n",
    "print(vectorizer.vocabulary_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gather\n",
      "gathering\n"
     ]
    }
   ],
   "source": [
    "# 词形还原（lemmatization）（考虑单词词性）\n",
    "# 是一种更精细的过程，它不仅去除词缀，还考虑单词的词性，将单词还原到其词典形式。\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "# WordNetLemmatizer 是一个基于 WordNet 数据库的词元还原器，它能够根据单词的词性（名词、动词、形容词或副词）来还原单词。\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize('gathering','v'))  # lemmatize 方法会查找 'gathering' 作为动词的基本形式\n",
    "print(lemmatizer.lemmatize('gathering','n'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gather\n"
     ]
    }
   ],
   "source": [
    "# 词干提取(删除词缀)\n",
    "# 目标是将单词减少到一个基本形式，通常不考虑词性，而是基于一系列规则去除单词的词缀\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "print(stemmer.stem('gathering'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对玩具语料库进行词干提取和词形还原"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 下载nltk的punkt数据包\n",
    "import nltk\n",
    "nltk.download() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed: [['he', 'ate', 'the', 'sandwich'], ['everi', 'sandwich', 'wa', 'eaten', 'by', 'him']]\n",
      "Lemmatized: [['He', 'eat', 'the', 'sandwich'], ['Every', 'sandwich', 'be', 'eat', 'by', 'him']]\n"
     ]
    }
   ],
   "source": [
    "# 词干提取： 1.对语料库分词 2.进行词干提取\n",
    "# 词形还原： 1.对语料库分词 2.得到词形标签 3.根据标签进行词形还原\n",
    "\n",
    "from nltk import word_tokenize # 函数用于将文本分割成单词，是 NLTK 提供的多种分词方法之一。  'tokenization' 分词  \n",
    "from nltk import pos_tag # 用于词性标注，给每个单词分配一个词形标签。 part-of-speech 词性\n",
    "from nltk.stem import PorterStemmer #用于词干提取，将单词还原到基本形式\n",
    "from nltk.stem.wordnet import WordNetLemmatizer # 用于词形还原，将单词还原到其词典形式。\n",
    "\n",
    "corpus = [\n",
    "    'He ate the sandwiches',\n",
    "    'Every sandwich was eaten by him'\n",
    "]\n",
    "\n",
    "# 词干提取\n",
    "# word_tokenize 用于分词\n",
    "# stemmer.stem 用于提取词干\n",
    "stemmer = PorterStemmer()\n",
    "\"\"\"\n",
    "使用了列表推导式(list comprehension)来创建一个嵌套列表\n",
    " 先运行 for document in corpus 再运行 for token in word_tokenize(document)\n",
    " \"\"\"\n",
    "print('Stemmed:', [[stemmer.stem(token) for token in word_tokenize(document)] for document in corpus])\n",
    "# 词形还原\n",
    "wordnet_tags = ['n', 'v']  # 词性标签，用于词形还原。\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize(token, tag):\n",
    "    \"\"\" \n",
    "    这个函数接受一个单词(token)和一个词性标签(tag)\n",
    "    如果标签是名词或动词，它将使用 lemmatizer.lemmatize 方法进行词形还原；\n",
    "    否则，它将返回原始单词。 \n",
    "    \"\"\"\n",
    "    if tag[0].lower() in wordnet_tags:\n",
    "        return lemmatizer.lemmatize(token, tag[0].lower()) # tag[0].lower()得到'n'或者'v', tag得到'NN'或者'VB'\n",
    "    return token\n",
    "\n",
    "\n",
    "# 分词并且贴词性标签\n",
    "tagged_corpus = [pos_tag(word_tokenize(document)) for document in corpus]\n",
    "#　调用lemmatize函数，对单词进行还原\n",
    "print('Lemmatized:', [[lemmatize(token, tag) for token, tag in document] for document in tagged_corpus])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stemmed: [['he', 'ate', 'the', 'sandwich'], ['everi', 'sandwich', 'wa', 'eaten', 'by', 'him']]\n",
      "lemmatized: [['He', 'eat', 'the', 'sandwich'], ['Every', 'sandwich', 'be', 'eat', 'by', 'him']]\n"
     ]
    }
   ],
   "source": [
    "# 提高可读性\n",
    "# 词干提取\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_corpus = [] # 处理后的语料库\n",
    "for document in corpus:\n",
    "    \"\"\"分词\"\"\"\n",
    "    tokens = word_tokenize(document)\n",
    "    stemmed_document = [] # 处理后的文档\n",
    "    for token in tokens:\n",
    "        \"\"\"词干提取\"\"\"\n",
    "        stemmed_token = stemmer.stem(token)\n",
    "        stemmed_document.append(stemmed_token)\n",
    "    stemmed_corpus.append(stemmed_document)\n",
    "print('stemmed:',stemmed_corpus)\n",
    "\n",
    "# 词形还原\n",
    "wordnet_tag = ['n','v']  # 标签\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(token, tag):\n",
    "    \"\"\"\n",
    "    接受单词及其标签\n",
    "    如果是动词或者名词，则进行相应的词形还原并且返回\n",
    "    否则返回原词\n",
    "    \"\"\"\n",
    "    if tag[0].lower() in wordnet_tag:\n",
    "        return lemmatizer.lemmatize(token,tag[0].lower())\n",
    "    return token\n",
    "\n",
    "\n",
    "# 分词、贴标签，进行词形还原\n",
    "lemmatized_corpus = []\n",
    "for document in corpus:\n",
    "    # 分词、贴标签\n",
    "    tagged_tokens = []\n",
    "    tagged_tokens = pos_tag(word_tokenize(document))\n",
    "\n",
    "    lemmatized_document = []\n",
    "    for token,tag in tagged_tokens:\n",
    "        # 进行词形还原\n",
    "        lemmatized_document.append(lemmatize(token,tag))\n",
    "    lemmatized_corpus.append(lemmatized_document)\n",
    "\n",
    "print('lemmatized:', lemmatized_corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ti-idf 权重扩展包\n",
    "1.使用一个整数来表示单词在文档中出现的次数，而不是使用一个二元值表示特征向量中的每个元素  \n",
    "2.tf：单词的频数 idf：逆文档频率   tf-idf：tf * idf   \n",
    "3.使用TfidfVectorizer类，其封装了CountVectorizer类和TfidfTransformer类\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 3 1 1]\n",
      "Token indices {'dog': 1, 'ate': 0, 'sandwich': 2, 'wozard': 4, 'transfigured': 3}\n",
      "The token \"dog\" appears 1 times\n",
      "The token \"ate\" appears 2 times\n",
      "The token \"sandwich\" appears 3 times\n",
      "The token \"wozard\" appears 1 times\n",
      "The token \"transfigured\" appears 1 times\n"
     ]
    }
   ],
   "source": [
    "#　计算一个文档里单词的频数\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = ['The dog ate a sandwich, the wozard transfigured a sandwich, and I ate a sandwich']\n",
    "vectorizer = CountVectorizer(stop_words='english') # binary参数默认为False，将返回单词出现的真正频数而不是一个二元值\n",
    "frequencies = np.array(vectorizer.fit_transform(corpus).todense())[0]  # 获取词频列表，由于文档只有一行，则列表只有一行\n",
    "print(frequencies)\n",
    "print('Token indices %s' % vectorizer.vocabulary_)\n",
    "for token, index in vectorizer.vocabulary_.items():\n",
    "    # 获取某词及其索引\n",
    "    print('The token \"%s\" appears %s times' % (token,frequencies[index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.75458397 0.37729199 0.53689271 0.         0.        ]\n",
      " [0.         0.         0.44943642 0.6316672  0.6316672 ]]\n"
     ]
    }
   ],
   "source": [
    "# 标准化单词频数\n",
    "# 计算每个单词的tf-idf值\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'The dog ate a sandwich and I ate a sandwich',\n",
    "    'The wizard transfigured a sandwich'\n",
    "]\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "print(vectorizer.fit_transform(corpus).todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 空间有效特征向量化与哈希技巧 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
